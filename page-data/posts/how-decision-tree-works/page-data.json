{"componentChunkName":"component---src-templates-post-template-tsx","path":"/posts/how-decision-tree-works","result":{"data":{"markdownRemark":{"html":"<p>A decision tree is so widely used and its idea is very simple. However, I always tend to forget how it works because it's too simple!\nDecision tree uses something called <strong>Information Gain</strong> as its objective. It tries to maximize the <strong>Information Gain</strong>.</p>\n<h2>Entropy</h2>\n<p>Before talking about information gain, we must know <strong>entropy</strong>.</p>\n<p><span class=\"math math-inline\">\\text{entropy} = - \\sum_{i} p_i \\log_2 p_i</span></p>\n<p>Its graph would look as below:\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/200px-Binary_entropy_plot.svg.png\" alt=\"Entropy Function Graph\"></p>\n<p>Notice that the entropy has its maximum at <span class=\"math math-inline\">p_i = 0.5</span></p>\n<h3>Example</h3>\n<table>\n<thead>\n<tr>\n<th>Grade</th>\n<th>Bumpiness</th>\n<th>Speed limit</th>\n<th><span class=\"text-danger\">Speed</span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Steep</td>\n<td>Bumpy</td>\n<td>Yes</td>\n<td><span class=\"text-danger\">Slow</span></td>\n</tr>\n<tr>\n<td>Steep</td>\n<td>Smooth</td>\n<td>Yes</td>\n<td><span class=\"text-danger\">Slow</span></td>\n</tr>\n<tr>\n<td>Flat</td>\n<td>Bumpy</td>\n<td>No</td>\n<td><span class=\"text-primary\">Fast</span></td>\n</tr>\n<tr>\n<td>Steep</td>\n<td>Smooth</td>\n<td>No</td>\n<td><span class=\"text-primary\">Fast</span></td>\n</tr>\n</tbody>\n</table>\n<p>Suppose we want to predict <strong>speed</strong>. Either <em>slow</em> or <em>fast</em>.</p>\n<p>The base entropy is 1 because</p>\n<div class=\"math math-display\">\\begin{aligned}\np_{slow} &#x26;= 0.5 \\\\\np_{fast} &#x26;= 0.5 \\\\\n\\end{aligned}</div>\n<div class=\"math math-display\">\\begin{aligned}\n\\text{entropy} &#x26;= - p_{slow} \\log_2 p_{slow} - p_{fast} \\log_2 p_{fast} \\\\\n               &#x26;= - 0.5 \\log_2 0.5 - 0.5 \\log_2 0.5 \\\\\n               &#x26;= - \\log_2 0.5 = 1.0\n\\end{aligned}</div>\n<h2>Information Gain</h2>\n<p>Finally, we can talk about the information gain. Information gain refers to a difference between parent and children entropy.</p>\n<p><span class=\"math math-inline\">\\text{Information Gain} = \\text{base entropy} - \\text{weighted average of children entropy}</span></p>\n<h3>Example</h3>\n<p>We have 3 features (<em>Grade</em>, <em>Bumpiness</em>, <em>Speed Limit</em>) and for each feature, we compute its entropy and compute the difference.</p>\n<p>In case of <em>Grade</em>,</p>\n<table>\n<thead>\n<tr>\n<th><span class=\"text-danger\">Grade</span></th>\n<th>Bumpiness</th>\n<th>Speed limit</th>\n<th>Speed</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><span class=\"text-warning\">Steep</span></td>\n<td>Bumpy</td>\n<td>Yes</td>\n<td><span class=\"text-danger\">Slow</span></td>\n</tr>\n<tr>\n<td><span class=\"text-warning\">Steep</span></td>\n<td>Smooth</td>\n<td>Yes</td>\n<td><span class=\"text-danger\">Slow</span></td>\n</tr>\n<tr>\n<td><span class=\"text-success\">Flat </span></td>\n<td>Bumpy</td>\n<td>No</td>\n<td><span class=\"text-primary\">Fast</span></td>\n</tr>\n<tr>\n<td><span class=\"text-warning\">Steep</span></td>\n<td>Smooth</td>\n<td>No</td>\n<td><span class=\"text-primary\">Fast</span></td>\n</tr>\n</tbody>\n</table>\n<img src=\"/2acb12a3c1270c40a1452019b3ee5a9c/decision-tree-example.svg\" alt=\"decision tree\">\n<p>we have two cases when the grade is <em>steep</em> or <em>flat</em></p>\n<h4>When grade is <code class=\"language-text\">steep</code></h4>\n<p>There are 2 slow and 1 fast examples. Therefore,</p>\n<div class=\"math math-display\">p_{slow} = \\frac{2}{3}</div>\n<div class=\"math math-display\">p_{fast} = \\frac{1}{3}</div>\n<div class=\"math math-display\">\\begin{aligned}\n\\text{entropy} &#x26;= - p_{slow} \\log_2 p_{slow} - p_{fast} \\log_2 p_{fast} \\\\\n               &#x26;\\approx 0.9184\n\\end{aligned}</div>\n<h4>When grade is <code class=\"language-text\">flat</code></h4>\n<p>It's simple because it's \"pure\" (there exists only one class). Its entropy is 0.</p>\n<h4>Information Gain</h4>\n<p>Therefore,</p>\n<p>the weighted average of children entropy(<span class=\"math math-inline\">e_1</span>) is</p>\n<p><span class=\"math math-inline\">e_1 = \\frac{3}{4} \\cdot 0.9184 + \\frac{1}{4} \\cdot 0</span></p>\n<p>the information gain</p>\n<p><span class=\"math math-inline\">\\text{Information Gain} = 1.0 - e_1 \\approx 0.3112</span></p>\n<p>If we repeat for other features (<em>Bumpiness</em>, <em>Speed Limit</em>)</p>\n<div class=\"math math-display\">\\text{Information Gain for \\textbf{grade}} = 0.311</div>\n<div class=\"math math-display\">\\text{Information Gain for \\textbf{bumpiness}} = 0</div>\n<div class=\"math math-display\">\\text{Information Gain for \\textbf{speed limit}}= 1</div>\n<p>Therefore, the next split will occur around <strong>Speed Limit</strong> because it has the greatest information gain.</p>\n<h2>Min Split</h2>\n<p>The decision tree will repeat the above process, but a question is when will stop splitting.\nThere are two popular hyperparameters for decision trees. One is <em>depth</em> and the other one is <em>min split</em>.</p>\n<p><em>Depth</em> refers to the depth of a tree. It will stop if current depth is at the max depth.</p>\n<p><em>Min split</em> refers to the number of minimum samples to split. For example, if a <code class=\"language-text\">min split</code> is 3 and there are 2 samples left.\nThere will be no split afterward.</p>\n<h2>Code Samples</h2>\n<p>In <code class=\"language-text\">sklearn</code>,</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn <span class=\"token keyword\">import</span> tree\nclf <span class=\"token operator\">=</span> tree<span class=\"token punctuation\">.</span>DecisionTreeClassifier<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nclf <span class=\"token operator\">=</span> clf<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> Y<span class=\"token punctuation\">)</span></code></pre></div>","excerpt":"A decision tree is so widely used and its idea is very simple. However, I always tend to forget how it works because it's too simpleâ€¦","frontmatter":{"date":"December 12, 2017","title":"How Decision Tree Works","keywords":null,"description":null},"fields":{"slug":"posts/how-decision-tree-works"}}},"pageContext":{"slug":"posts/how-decision-tree-works"}},"staticQueryHashes":[]}